{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load model.py\n",
    "from math import sin, cos\n",
    "import numpy as np\n",
    "from model_parameters import m, g, I, l, Kt, Kd\n",
    "\n",
    "def transfomationMatrix(roll, pitch, yaw):\n",
    "    cR = cos(roll)\n",
    "    cP = cos(pitch)\n",
    "    cY = cos(yaw)\n",
    "    sR = sin(roll)\n",
    "    sP = sin(pitch)\n",
    "    sY = sin(yaw)\n",
    "    R = np.array([[cP*cY, sR*sP*cY - cR*sY, cR*sP*cY + sR*sY],\n",
    "                  [cP*sY, sR*sP*sY - cR*cY, cR*sP*sY - sR*cY],\n",
    "                  [-sP, sR*cP, cR*cP]], dtype=np.float32)\n",
    "    return R\n",
    "\n",
    "def translationalMotion(R, F):\n",
    "    f = F[0] + F[1]  + F[2] + F[3]\n",
    "    accelerations = np.zeros((3, 1), dtype=np.float32)\n",
    "    bodyFrameThrust = np.array([[0], \n",
    "                               [0],\n",
    "                               [f]], dtype=np.float32)\n",
    "    referenceFrame = np.matmul(R, bodyFrameThrust) + np.array([[0],\n",
    "                                                               [0],\n",
    "                                                               [-m*g]], dtype=np.float32)\n",
    "    accelerations = referenceFrame/m\n",
    "    accelerations = np.reshape(accelerations, 3)\n",
    "    return accelerations\n",
    "\n",
    "def angularMotion(F, M, eulerAnglesPrim):\n",
    "    T = np.zeros(3)\n",
    "    T[0] = (F[0] - F[3])*l\n",
    "    T[1] = (F[1] - F[2])*l\n",
    "    T[2] = M[0] + M[1] + M[2] + M[3]\n",
    "    accelerations =  T - np.cross(eulerAnglesPrim, np.multiply(I, eulerAnglesPrim))\n",
    "    accelerations = np.divide(accelerations, I)\n",
    "    return accelerations\n",
    "\n",
    "def inputToForces(omega):\n",
    "    return np.multiply(Kt, omega)\n",
    "\n",
    "def inputToMomentum(omega):\n",
    "    return np.multiply(Kd, omega)\n",
    "\n",
    "def model(x):\n",
    "    dstate = np.zeros(12)\n",
    "    omega = np.array([2000, 2000, 2000, 2000])\n",
    "    F = inputToForces(omega)\n",
    "    M = inputToMomentum(omega)\n",
    "    R = transfomationMatrix(x[3], x[4], x[5])\n",
    "    dstate[0:3] = x[3:6]\n",
    "    dstate[3:6] = translationalMotion(R, F)\n",
    "    dstate[6:9] = x[9:12]\n",
    "    dstate[9:12] = angularMotion(F, M, x[9:12])\n",
    "    return dstate \n",
    "\n",
    "def modelRT(x, u, deltaT):\n",
    "    state = np.zeros(12)\n",
    "    omega = np.array(u)\n",
    "    F = inputToForces(omega)\n",
    "    M = inputToMomentum(omega)\n",
    "    R = transfomationMatrix(x[3], x[4], x[5])\n",
    "    state[0:3] = x[3:6] * deltaT + x[0:3]\n",
    "    state[3:6] = translationalMotion(R, F) * deltaT + x[3:6]\n",
    "    state[6:9] = x[9:12] * deltaT + x[6:9]\n",
    "    state[9:12] = angularMotion(F, M, x[9:12]) * deltaT + x[9:12]\n",
    "    return state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroneEnv(Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = Box(low=0, high=2000, shape=(4,), dtype=np.float32)\n",
    "        self.observation_space = Box(low=-inf, high=inf, shape=(12,))\n",
    "        self.state = np.zeros(12)\n",
    "        self.state[2] = 5\n",
    "        self.maximum_angle = radians(30)\n",
    "        self.flight_length = 10\n",
    "        \n",
    "    def step(self, action, deltaT):\n",
    "        self.state = modelRT(self.state, action, deltaT)\n",
    "        self.flight_lenght = self.flight_length-deltaT\n",
    "\n",
    "        reward = - abs(5 - self.state[2])\n",
    "        print(reward)\n",
    "        if self.state[2] < 0:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        info = {}\n",
    "        return self.state, reward, done, info\n",
    "        \n",
    "    def render(self):\n",
    "        #visualization\n",
    "        pass\n",
    "    def reset(self):\n",
    "        self.state = np.zeros(12)\n",
    "        self.state[2] = 5\n",
    "        self.flight_lenght = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DroneEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1631.0653,  720.1694,  653.44  , 1608.3169], dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0\n",
      "-0.09799790382385254\n",
      "-0.2939600884914402\n",
      "-0.5879106581211095\n",
      "-0.9798849284648901\n",
      "-1.4698523998260504\n",
      "-2.057761108875275\n",
      "-2.743661999702454\n",
      "-3.5275483608245857\n",
      "-4.409421390295029\n",
      "-5.389251118898392\n",
      "Episode:1 Score:-21.55724995732308\n",
      "-0.0\n",
      "-0.09796308875083959\n",
      "-0.2939268290996555\n",
      "-0.5878638625144958\n",
      "-0.9798436820507046\n",
      "-1.4697650134563442\n",
      "-2.0576595902442927\n",
      "-2.7435324430465693\n",
      "-3.5273723125457757\n",
      "-4.4091506421566\n",
      "-5.388855654001236\n",
      "Episode:2 Score:-21.555933117866513\n",
      "-0.0\n",
      "-0.09792694449424744\n",
      "-0.2938214480876926\n",
      "-0.5877285420894625\n",
      "-0.9795751571655273\n",
      "-1.4693969428539275\n",
      "-2.057179409265518\n",
      "-2.742942517995834\n",
      "-3.526697820425033\n",
      "-4.40844486951828\n",
      "-5.38813636302948\n",
      "Episode:3 Score:-21.551850014925\n",
      "-0.0\n",
      "-0.09803283810615504\n",
      "-0.2940421998500824\n",
      "-0.5880227565765379\n",
      "-0.9800088405609131\n",
      "-1.4699857890605927\n",
      "-2.0579434573650364\n",
      "-2.743893736600876\n",
      "-3.527839785814286\n",
      "-4.409762006998063\n",
      "-5.389676487445832\n",
      "Episode:4 Score:-21.559207898378375\n",
      "-0.0\n",
      "-0.09797388911247218\n",
      "-0.2939375579357142\n",
      "-0.5878683447837822\n",
      "-0.9798046469688408\n",
      "-1.469764214754104\n",
      "-2.0576987981796258\n",
      "-2.7435507416725153\n",
      "-3.527386218309402\n",
      "-4.409139913320541\n",
      "-5.3889129042625425\n",
      "Episode:5 Score:-21.55603722929954\n",
      "-0.0\n",
      "-0.09798595309257507\n",
      "-0.2939130604267124\n",
      "-0.5878207743167883\n",
      "-0.9797082901000982\n",
      "-1.4695740342140202\n",
      "-2.05743197798729\n",
      "-2.74331061244011\n",
      "-3.527247118949891\n",
      "-4.409142184257508\n",
      "-5.389039301872254\n",
      "Episode:6 Score:-21.55517330765725\n",
      "-0.0\n",
      "-0.09792677164077723\n",
      "-0.29385058879852277\n",
      "-0.5877455592155458\n",
      "-0.9795606553554537\n",
      "-1.4693404197692872\n",
      "-2.0570578277111053\n",
      "-2.742741161584854\n",
      "-3.526375412940979\n",
      "-4.407955253124237\n",
      "-5.387521094083786\n",
      "Episode:7 Score:-21.550074744224553\n",
      "-0.0\n",
      "-0.09793233871459961\n",
      "-0.2938422679901125\n",
      "-0.5877622425556188\n",
      "-0.9796779572963716\n",
      "-1.4696045100688937\n",
      "-2.057493579387665\n",
      "-2.743415677547455\n",
      "-3.5273221969604496\n",
      "-4.409212946891785\n",
      "-5.389099115133286\n",
      "Episode:8 Score:-21.555362832546237\n",
      "-0.0\n",
      "-0.09792838096618617\n",
      "-0.29382457733154244\n",
      "-0.5876654744148251\n",
      "-0.9795087695121758\n",
      "-1.4693545162677757\n",
      "-2.0571697592735285\n",
      "-2.7429043054580684\n",
      "-3.5265995144844053\n",
      "-4.408237582445144\n",
      "-5.387877482175827\n",
      "Episode:9 Score:-21.55107036232948\n",
      "-0.0\n",
      "-0.09798084497451764\n",
      "-0.2939549028873438\n",
      "-0.5878884613513939\n",
      "-0.9798588931560506\n",
      "-1.4697959780693042\n",
      "-2.057706475257872\n",
      "-2.743593132495879\n",
      "-3.5274625301361073\n",
      "-4.409295290708541\n",
      "-5.389128321409224\n",
      "Episode:10 Score:-21.556664830446234\n",
      "-0.0\n",
      "-0.09795012474060094\n",
      "-0.29387251138687187\n",
      "-0.5877763926982889\n",
      "-0.9796594381332406\n",
      "-1.4695609390735633\n",
      "-2.0574281215667733\n",
      "-2.7432713985443122\n",
      "-3.5271242439746864\n",
      "-4.4089395105838785\n",
      "-5.388742780685425\n",
      "Episode:11 Score:-21.55432546138764\n",
      "-0.0\n",
      "-0.09797331094741857\n",
      "-0.293946701288224\n",
      "-0.5878456532955179\n",
      "-0.9797045171260841\n",
      "-1.4695189893245706\n",
      "-2.0573686718940745\n",
      "-2.7432037591934213\n",
      "-3.5270135402679452\n",
      "-4.408818155527116\n",
      "-5.388613122701646\n",
      "Episode:12 Score:-21.554006421566015\n",
      "-0.0\n",
      "-0.09798054695129377\n",
      "-0.2939709305763243\n",
      "-0.5879555940628052\n",
      "-0.9798847258090975\n",
      "-1.46975069642067\n",
      "-2.057621598243714\n",
      "-2.7434608995914465\n",
      "-3.5272288680076604\n",
      "-4.408966749906541\n",
      "-5.38872110247612\n",
      "Episode:13 Score:-21.555541712045674\n",
      "-0.0\n",
      "-0.09799546599388087\n",
      "-0.2940099060535424\n",
      "-0.5879668533802027\n",
      "-0.9798624277114865\n",
      "-1.469733703136444\n",
      "-2.0575835824012754\n",
      "-2.7434595286846157\n",
      "-3.527297919988632\n",
      "-4.409176075458526\n",
      "-5.388974344730377\n",
      "Episode:14 Score:-21.55605980753898\n",
      "-0.0\n",
      "-0.0979049801826477\n",
      "-0.29379643797874433\n",
      "-0.5875986158847804\n",
      "-0.9793570339679709\n",
      "-1.469091755151748\n",
      "-2.0568160474300377\n",
      "-2.7425130009651175\n",
      "-3.526229298114776\n",
      "-4.407961219549178\n",
      "-5.387666106224059\n",
      "Episode:15 Score:-21.54893449544906\n",
      "-0.0\n",
      "-0.0979650616645813\n",
      "-0.2939384162425993\n",
      "-0.5879330694675442\n",
      "-0.9798715233802788\n",
      "-1.469750547409057\n",
      "-2.0576217949390405\n",
      "-2.743445593118667\n",
      "-3.5272560060024256\n",
      "-4.409068197011948\n",
      "-5.388882625102997\n",
      "Episode:16 Score:-21.55573283433914\n",
      "-0.0\n",
      "-0.09803930521011317\n",
      "-0.29410688281059194\n",
      "-0.5881502389907833\n",
      "-0.9801606774330134\n",
      "-1.4701915919780726\n",
      "-2.0582350671291345\n",
      "-2.744226944446563\n",
      "-3.5281988918781275\n",
      "-4.410180491209029\n",
      "-5.3901214599609375\n",
      "Episode:17 Score:-21.561611551046365\n",
      "-0.0\n",
      "-0.09801054000854492\n",
      "-0.2940076053142544\n",
      "-0.5880289912223811\n",
      "-0.9800025522708884\n",
      "-1.4699433505535118\n",
      "-2.0578512609004966\n",
      "-2.743782120943069\n",
      "-3.527634763717651\n",
      "-4.409481412172317\n",
      "-5.3892815709114075\n",
      "Episode:18 Score:-21.55802416801452\n",
      "-0.0\n",
      "-0.09798431396484375\n",
      "-0.29397435188293475\n",
      "-0.5880421638488773\n",
      "-0.9800495743751529\n",
      "-1.470065486431122\n",
      "-2.058114266395569\n",
      "-2.7441716969013217\n",
      "-3.5281280875206\n",
      "-4.4100512921810155\n",
      "-5.389917290210724\n",
      "Episode:19 Score:-21.560498523712162\n",
      "-0.0\n",
      "-0.09792407155036909\n",
      "-0.2938255250453947\n",
      "-0.5877155065536499\n",
      "-0.9795369505882263\n",
      "-1.4693969190120697\n",
      "-2.0572278022766115\n",
      "-2.7430014193058017\n",
      "-3.526765447854996\n",
      "-4.408492857217789\n",
      "-5.388218641281128\n",
      "Episode:20 Score:-21.552105140686038\n"
     ]
    }
   ],
   "source": [
    "episodes = 20\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action, 0.1)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'. format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    model.add(Flatten())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNNmodel = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_13 (Dense)            (None, 24)                312       \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 24)                600       \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 4)                 100       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,012\n",
      "Trainable params: 1,012\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNNmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy \n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildAgent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=10,\n",
    "                   target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Keras symbolic inputs/outputs do not implement `__len__`. You may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model. This error will also get raised if you try asserting a symbolic input/output directly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/pete/Documents/repository/AdaptiveDrone/Simulation/reinforcement_learning.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/pete/Documents/repository/AdaptiveDrone/Simulation/reinforcement_learning.ipynb#ch0000014?line=0'>1</a>\u001b[0m dqn \u001b[39m=\u001b[39m buildAgent(CNNmodel, actions)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pete/Documents/repository/AdaptiveDrone/Simulation/reinforcement_learning.ipynb#ch0000014?line=1'>2</a>\u001b[0m dqn\u001b[39m.\u001b[39mcompile(Adam(lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m), metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pete/Documents/repository/AdaptiveDrone/Simulation/reinforcement_learning.ipynb#ch0000014?line=2'>3</a>\u001b[0m dqn\u001b[39m.\u001b[39mfit(env, nb_steps\u001b[39m=\u001b[39m\u001b[39m50000\u001b[39m, visualize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/home/pete/Documents/repository/AdaptiveDrone/Simulation/reinforcement_learning.ipynb Cell 15\u001b[0m in \u001b[0;36mbuildAgent\u001b[0;34m(model, actions)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pete/Documents/repository/AdaptiveDrone/Simulation/reinforcement_learning.ipynb#ch0000014?line=1'>2</a>\u001b[0m policy \u001b[39m=\u001b[39m BoltzmannQPolicy()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pete/Documents/repository/AdaptiveDrone/Simulation/reinforcement_learning.ipynb#ch0000014?line=2'>3</a>\u001b[0m memory \u001b[39m=\u001b[39m SequentialMemory(limit\u001b[39m=\u001b[39m\u001b[39m50000\u001b[39m, window_length\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/pete/Documents/repository/AdaptiveDrone/Simulation/reinforcement_learning.ipynb#ch0000014?line=3'>4</a>\u001b[0m dqn \u001b[39m=\u001b[39m DQNAgent(model, memory\u001b[39m=\u001b[39;49mmemory, policy\u001b[39m=\u001b[39;49mpolicy, nb_actions\u001b[39m=\u001b[39;49mactions, nb_steps_warmup\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pete/Documents/repository/AdaptiveDrone/Simulation/reinforcement_learning.ipynb#ch0000014?line=4'>5</a>\u001b[0m                target_model_update\u001b[39m=\u001b[39;49m\u001b[39m1e-2\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pete/Documents/repository/AdaptiveDrone/Simulation/reinforcement_learning.ipynb#ch0000014?line=5'>6</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dqn\n",
      "File \u001b[0;32m~/anaconda3/envs/AdaptiveDrone/lib/python3.8/site-packages/rl/agents/dqn.py:108\u001b[0m, in \u001b[0;36mDQNAgent.__init__\u001b[0;34m(self, model, policy, test_policy, enable_double_dqn, enable_dueling_network, dueling_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39msuper\u001b[39m(DQNAgent, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    107\u001b[0m \u001b[39m# Validate (important) input.\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model\u001b[39m.\u001b[39moutput, \u001b[39m'\u001b[39m\u001b[39m__len__\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39;49m(model\u001b[39m.\u001b[39;49moutput) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    109\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mModel \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m has more than one output. DQN expects a model that has a single output.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(model))\n\u001b[1;32m    110\u001b[0m \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39m_keras_shape \u001b[39m!=\u001b[39m (\u001b[39mNone\u001b[39;00m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_actions):\n",
      "File \u001b[0;32m~/anaconda3/envs/AdaptiveDrone/lib/python3.8/site-packages/keras/engine/keras_tensor.py:221\u001b[0m, in \u001b[0;36mKerasTensor.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 221\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mKeras symbolic inputs/outputs do not \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    222\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mimplement `__len__`. You may be \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    223\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mtrying to pass Keras symbolic inputs/outputs \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    224\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mto a TF API that does not register dispatching, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    225\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mpreventing Keras from automatically \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    226\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mconverting the API call to a lambda layer \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    227\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39min the Functional Model. This error will also get raised \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    228\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mif you try asserting a symbolic input/output directly.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Keras symbolic inputs/outputs do not implement `__len__`. You may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model. This error will also get raised if you try asserting a symbolic input/output directly."
     ]
    }
   ],
   "source": [
    "dqn = buildAgent(CNNmodel, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('AdaptiveDrone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "066beb5b506c9b2b302fc49ae2a0347e44d1fc406c5708fca4606113c3d0b8ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
